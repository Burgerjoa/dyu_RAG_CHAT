"""
ë™ì–‘ëŒ€í•™êµ RAG ê¸°ë°˜ ì±—ë´‡ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ LangChain, OpenAI API, ChromaDBë¥¼ ì‚¬ìš©í•˜ì—¬
í•™ìƒë“¤ì˜ í•™ì‚¬ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” RAG ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import os
from typing import Dict, List, Any
from pathlib import Path

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class RAGSystem:
    """RAG ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ"""

    def __init__(self, data_path: str = "data/sample_data.json",
                 vectorstore_path: str = "vectorstore"):
        """
        RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”

        Args:
            data_path: JSON ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            vectorstore_path: ë²¡í„° DB ì €ì¥ ê²½ë¡œ
        """
        print("ğŸš€ RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘...")

        self.data_path = data_path
        self.vectorstore_path = vectorstore_path

        # OpenAI API í‚¤ í™•ì¸
        if not os.getenv("OPENAI_API_KEY"):
            raise ValueError("âŒ OPENAI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

        # OpenAI Embeddings ì„¤ì •
        print("ğŸ“ ì„ë² ë”© ëª¨ë¸ ì„¤ì • ì¤‘ (text-embedding-3-small)...")
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"
        )

        # OpenAI LLM ì„¤ì •
        print("ğŸ¤– LLM ëª¨ë¸ ì„¤ì • ì¤‘ (gpt-4o-mini)...")
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.3
        )

        # ë²¡í„° DB ë¡œë“œ ë˜ëŠ” ìƒì„±
        self.vectorstore = self._load_or_create_vectorstore()

        # QA ì²´ì¸ ìƒì„±
        print("âš™ï¸ QA ì²´ì¸ ìƒì„± ì¤‘...")
        self.qa_chain = self._create_qa_chain()

        print("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!\n")

    def _load_or_create_vectorstore(self) -> Chroma:
        """
        ë²¡í„° DB ë¡œë“œ ë˜ëŠ” ìƒì„±

        Returns:
            Chroma ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
        """
        if os.path.exists(self.vectorstore_path):
            print("ğŸ“‚ ê¸°ì¡´ ë²¡í„° DBë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
            return Chroma(
                persist_directory=self.vectorstore_path,
                embedding_function=self.embeddings
            )
        else:
            print("ğŸ”¨ ìƒˆë¡œìš´ ë²¡í„° DBë¥¼ ìƒì„±í•˜ëŠ” ì¤‘...")
            return self._create_vectorstore()

    def _create_vectorstore(self) -> Chroma:
        """
        JSON ë°ì´í„°ë¡œë¶€í„° ë²¡í„° DB ìƒì„±

        Returns:
            Chroma ë²¡í„°ìŠ¤í† ì–´ ì¸ìŠ¤í„´ìŠ¤
        """
        # JSON ë°ì´í„° ë¡œë“œ
        print(f"ğŸ“– ë°ì´í„° íŒŒì¼ ë¡œë“œ ì¤‘: {self.data_path}")
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        print(f"ğŸ“Š ì´ {len(data)}ê°œì˜ ë¬¸ì„œë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

        # Document ê°ì²´ ìƒì„±
        documents = []
        for item in data:
            doc = Document(
                page_content=item['content'],
                metadata={
                    'source': item['url'],
                    'title': item['title']
                }
            )
            documents.append(doc)

        # í…ìŠ¤íŠ¸ ë¶„í• 
        print("âœ‚ï¸ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ì¤‘...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=100,
            length_function=len
        )
        splits = text_splitter.split_documents(documents)
        print(f"ğŸ“ ì´ {len(splits)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ChromaDBì— ë°°ì¹˜ë¡œ ì €ì¥ (OpenAI API í† í° ì œí•œ íšŒí”¼)
        print("ğŸ’¾ ë²¡í„° DBì— ì €ì¥í•˜ëŠ” ì¤‘ (ë°°ì¹˜ ì²˜ë¦¬)...")

        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ vectorstore ìƒì„±
        batch_size = 100  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜

        if len(splits) > 0:
            # ì²« ë°°ì¹˜ë¡œ vectorstore ì´ˆê¸°í™”
            first_batch = splits[:batch_size]
            vectorstore = Chroma.from_documents(
                documents=first_batch,
                embedding=self.embeddings,
                persist_directory=self.vectorstore_path
            )
            print(f"  âœ“ ë°°ì¹˜ 1/{(len(splits)-1)//batch_size + 1} ì™„ë£Œ ({len(first_batch)}ê°œ ì²­í¬)")

            # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì¶”ê°€
            for i in range(batch_size, len(splits), batch_size):
                batch = splits[i:i+batch_size]
                vectorstore.add_documents(batch)
                batch_num = i//batch_size + 1
                total_batches = (len(splits)-1)//batch_size + 1
                print(f"  âœ“ ë°°ì¹˜ {batch_num}/{total_batches} ì™„ë£Œ ({len(batch)}ê°œ ì²­í¬)")

        print(f"âœ… ë²¡í„° DB ìƒì„± ì™„ë£Œ! (ì €ì¥ ê²½ë¡œ: {self.vectorstore_path})")
        print(f"ğŸ“¦ ì´ {len(splits)}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return vectorstore

    def _create_qa_chain(self) -> RetrievalQA:
        """
        QA ì²´ì¸ ìƒì„±

        Returns:
            RetrievalQA ì²´ì¸ ì¸ìŠ¤í„´ìŠ¤
        """
        # í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        template = """ë‹¹ì‹ ì€ ë™ì–‘ëŒ€í•™êµì˜ ì¹œì ˆí•œ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
í•™ìƒë“¤ì˜ í•™ì‚¬ ê´€ë ¨ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¼ì£¼ì„¸ìš”:
1. ì£¼ì–´ì§„ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ì •ë³´ê°€ ì—†ìœ¼ë©´ "ì œê³µëœ ì •ë³´ì—ì„œëŠ” í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì •ì§í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
4. ê°€ëŠ¥í•œ ê²½ìš° êµ¬ì²´ì ì¸ ë‚ ì§œ, ì‹œê°„, ì—°ë½ì²˜ ë“±ì„ í¬í•¨í•˜ì„¸ìš”.

ì°¸ê³  ì •ë³´:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

        PROMPT = PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

        # RetrievalQA ì²´ì¸ êµ¬ì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": 3}  # Top-3 ë¬¸ì„œ ê²€ìƒ‰
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )

        return qa_chain

    def ask(self, question: str) -> Dict[str, Any]:
        """
        ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            ë‹µë³€ê³¼ ì¶œì²˜ë¥¼ í¬í•¨í•œ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\nâ“ ì§ˆë¬¸: {question}")
        print("ğŸ” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘...")

        # QA ì²´ì¸ ì‹¤í–‰
        result = self.qa_chain.invoke({"query": question})

        # ë‹µë³€ ì¶”ì¶œ
        answer = result['result']

        # ì¶œì²˜ ë¬¸ì„œ ì¶”ì¶œ
        sources = []
        for doc in result['source_documents']:
            sources.append({
                'title': doc.metadata.get('title', 'Unknown'),
                'source': doc.metadata.get('source', 'Unknown'),
                'content': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content
            })

        print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ!\n")

        return {
            'answer': answer,
            'sources': sources
        }

    def reset_vectorstore(self) -> None:
        """ë²¡í„° DB ì¬ìƒì„±"""
        print("ğŸ”„ ë²¡í„° DBë¥¼ ì¬ìƒì„±í•˜ëŠ” ì¤‘...")

        # ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ
        if os.path.exists(self.vectorstore_path):
            import shutil
            shutil.rmtree(self.vectorstore_path)
            print("ğŸ—‘ï¸ ê¸°ì¡´ ë²¡í„° DB ì‚­ì œ ì™„ë£Œ")

        # ìƒˆë¡œìš´ ë²¡í„° DB ìƒì„±
        self.vectorstore = self._create_vectorstore()

        # QA ì²´ì¸ ì¬ìƒì„±
        self.qa_chain = self._create_qa_chain()

        print("âœ… ë²¡í„° DB ì¬ìƒì„± ì™„ë£Œ!\n")


def main():
    """ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ (í„°ë¯¸ë„)"""
    print("=" * 60)
    print("ğŸ“ ë™ì–‘ëŒ€í•™êµ AI ë„ìš°ë¯¸")
    print("=" * 60)
    print()

    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        rag = RAGSystem()
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    print("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
    print("-" * 60)

    # ëŒ€í™” ë£¨í”„
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            question = input("\nì§ˆë¬¸: ").strip()

            # ì¢…ë£Œ ì¡°ê±´
            if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nğŸ‘‹ ë™ì–‘ëŒ€í•™êµ AI ë„ìš°ë¯¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            # ë¹ˆ ì…ë ¥ ì²˜ë¦¬
            if not question:
                print("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue

            # ë‹µë³€ ìƒì„±
            result = rag.ask(question)

            # ë‹µë³€ ì¶œë ¥
            print("\n" + "=" * 60)
            print("ğŸ’¡ ë‹µë³€:")
            print(result['answer'])

            # ì¶œì²˜ ì¶œë ¥
            if result['sources']:
                print("\nğŸ“š ì°¸ê³  ë¬¸ì„œ:")
                for i, source in enumerate(result['sources'], 1):
                    print(f"\n{i}. {source['title']}")
                    print(f"   ğŸ”— {source['source']}")

            print("=" * 60)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ë™ì–‘ëŒ€í•™êµ AI ë„ìš°ë¯¸ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
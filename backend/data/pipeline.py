"""
ì „ì²´ ìë™í™” íŒŒì´í”„ë¼ì¸

1. ì›¹ í¬ë¡¤ë§ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
2. ì´ë¯¸ì§€ OCR (OpenAI Vision)
3. ë°ì´í„° ì •ì œ (HTML ì œê±°, ì¤‘ë³µ ì œê±°)
4. ìµœì¢… JSON ìƒì„±
"""

import json
import os
from datetime import datetime
from crawl_with_images import crawl_multiple_pages
from extract_image_text import ImageTextExtractor
from clean_data import clean_html, remove_duplicates, filter_low_quality, print_statistics


class Pipeline:
    """ë°ì´í„° íŒŒì´í”„ë¼ì¸"""

    def __init__(self, output_dir: str = "output"):
        """
        ì´ˆê¸°í™”

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # íƒ€ì„ìŠ¤íƒ¬í”„
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        print(f"ğŸš€ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”")
        print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}\n")

    def run(self, urls: list, ocr_enabled: bool = True, ocr_model: str = "gpt-4o-mini"):
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            urls: í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
            ocr_enabled: OCR ì‚¬ìš© ì—¬ë¶€
            ocr_model: OCR ëª¨ë¸ (gpt-4o ë˜ëŠ” gpt-4o-mini)

        Returns:
            ìµœì¢… ì²˜ë¦¬ëœ ë°ì´í„°
        """
        print("="*70)
        print(" "*20 + "ğŸ“Š íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("="*70 + "\n")

        # ========== 1ë‹¨ê³„: í¬ë¡¤ë§ ==========
        print("\n" + "="*70)
        print("1ï¸âƒ£ ì›¹ í¬ë¡¤ë§ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)")
        print("="*70 + "\n")

        data = crawl_multiple_pages(urls)

        # ì¤‘ê°„ ì €ì¥
        crawled_file = os.path.join(self.output_dir, f'step1_crawled_{self.timestamp}.json')
        self._save_json(data, crawled_file)
        print(f"âœ… í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥: {crawled_file}\n")

        # í†µê³„
        total_images = sum(d.get('image_count', 0) for d in data)
        successful = sum(1 for d in data if 'error' not in d and len(d.get('text', '')) > 0)
        failed = len(data) - successful
        avg_text_length = sum(len(d.get('text', '')) for d in data) / len(data) if data else 0

        print(f"ğŸ“Š í¬ë¡¤ë§ í†µê³„:")
        print(f"  - ì´ í˜ì´ì§€: {len(data)}ê°œ")
        print(f"  - ì„±ê³µ: {successful}ê°œ")
        print(f"  - ì‹¤íŒ¨: {failed}ê°œ")
        print(f"  - í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {avg_text_length:.0f}ì")
        print(f"  - ì´ë¯¸ì§€: {total_images}ê°œ")

        if failed > 0:
            print(f"\nâš ï¸ {failed}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨")
            for d in data:
                if 'error' in d or len(d.get('text', '')) == 0:
                    print(f"  - {d['url']}: {d.get('error', 'í…ìŠ¤íŠ¸ ì—†ìŒ')}")

        # ========== 2ë‹¨ê³„: OCR (ì„ íƒì‚¬í•­) ==========
        if ocr_enabled and total_images > 0:
            print("\n" + "="*70)
            print("2ï¸âƒ£ ì´ë¯¸ì§€ OCR (OpenAI Vision)")
            print("="*70 + "\n")

            # ë¹„ìš© ê³„ì‚°
            cost_per_1000 = {"gpt-4o": 10.0, "gpt-4o-mini": 3.0}
            estimated_cost = total_images * cost_per_1000[ocr_model] / 1000

            print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.2f} (ì•½ {estimated_cost*1300:.0f}ì›)")
            print(f"ğŸ“¸ ì²˜ë¦¬í•  ì´ë¯¸ì§€: {total_images}ê°œ")
            print(f"ğŸ¤– ì‚¬ìš© ëª¨ë¸: {ocr_model}\n")

            # í™•ì¸
            confirm = input("OCRì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()

            if confirm == 'y':
                extractor = ImageTextExtractor(model=ocr_model)
                data = extractor.process_images(data)

                # ì¤‘ê°„ ì €ì¥
                ocr_file = os.path.join(self.output_dir, f'step2_ocr_{self.timestamp}.json')
                self._save_json(data, ocr_file)
                print(f"âœ… OCR ê²°ê³¼ ì €ì¥: {ocr_file}\n")
            else:
                print("â­ï¸ OCR ìƒëµë¨\n")
        else:
            if not ocr_enabled:
                print("\nâ­ï¸ 2ï¸âƒ£ OCR ë¹„í™œì„±í™” (ìƒëµ)\n")
            else:
                print("\nâ­ï¸ 2ï¸âƒ£ ì´ë¯¸ì§€ ì—†ìŒ (OCR ìƒëµ)\n")

        # ========== 3ë‹¨ê³„: ë°ì´í„° ì •ì œ ==========
        print("\n" + "="*70)
        print("3ï¸âƒ£ ë°ì´í„° ì •ì œ")
        print("="*70 + "\n")

        original_data = [d.copy() for d in data]

        # 3-1. ì¤‘ë³µ ì œê±°
        print("  ğŸ”¸ ì¤‘ë³µ URL ì œê±° ì¤‘...")
        before = len(data)
        data = remove_duplicates(data)
        after = len(data)
        print(f"     ì œê±°: {before - after}ê°œ, ë‚¨ìŒ: {after}ê°œ\n")

        # 3-2. HTML ì •ì œ
        print("  ğŸ”¸ HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
        for i, item in enumerate(data):
            if i % 100 == 0 and i > 0:
                print(f"     ì§„í–‰: {i}/{len(data)} ({i/len(data)*100:.1f}%)")

            # content í•„ë“œ ì •ì œ
            if 'content' in item:
                item['content'] = clean_html(item['content'])
            # text í•„ë“œë„ ì •ì œ (ìˆë‹¤ë©´)
            if 'text' in item:
                item['text'] = clean_html(item['text'])

        print(f"     ì™„ë£Œ: {len(data)}ê°œ ë¬¸ì„œ ì •ì œ\n")

        # 3-3. í’ˆì§ˆ í•„í„°ë§
        print("  ğŸ”¸ ì €í’ˆì§ˆ ë¬¸ì„œ í•„í„°ë§ ì¤‘ (ìµœì†Œ 10ì)...")
        before = len(data)
        data = filter_low_quality(data, min_length=10)
        after = len(data)
        print(f"     ì œê±°: {before - after}ê°œ, ë‚¨ìŒ: {after}ê°œ\n")

        # ë°ì´í„°ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ê²½ê³ 
        if after == 0:
            print("\nâš ï¸ ê²½ê³ : ëª¨ë“  ë°ì´í„°ê°€ í•„í„°ë§ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ì›ì¸:")
            print("  1. í¬ë¡¤ë§ ì‹¤íŒ¨ (í˜ì´ì§€ ì ‘ê·¼ ë¶ˆê°€)")
            print("  2. ëª¨ë“  ë¬¸ì„œê°€ 10ì ë¯¸ë§Œ")
            print("  3. ì¤‘ë³µ ì œê±° ì‹œ ëª¨ë‘ ì œê±°ë¨")
            print("\ní•´ê²° ë°©ë²•:")
            print("  - URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
            print("  - í•™êµ ì„œë²„ ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸")
            print("  - min_length ê°’ì„ ë” ì‘ê²Œ ì¡°ì • (í˜„ì¬: 10)")
            print()

        # í†µê³„ ì¶œë ¥
        print_statistics(original_data, data)

        # ìµœì¢… ì €ì¥
        final_file = os.path.join(self.output_dir, f'final_data_{self.timestamp}.json')
        self._save_json(data, final_file)

        print(f"\nâœ… ìµœì¢… ë°ì´í„° ì €ì¥: {final_file}")

        # ========== ì™„ë£Œ ==========
        print("\n" + "="*70)
        print(" "*20 + "ğŸ‰ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("="*70)
        print(f"\nğŸ“ ì¶œë ¥ íŒŒì¼: {final_file}")
        print(f"ğŸ“Š ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(data)}ê°œ")
        print(f"\në‹¤ìŒ ë‹¨ê³„:")
        print(f"  1. backend/main.pyì—ì„œ data_path ë³€ê²½:")
        print(f"     data_path=\"../{final_file}\"")
        print(f"  2. vectorstore ì‚­ì œ í›„ ì„œë²„ ì¬ì‹œì‘")
        print("="*70 + "\n")

        return data

    def _save_json(self, data: list, filename: str):
        """JSON ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\n" + "="*70)
    print(" "*15 + "ë™ì–‘ëŒ€í•™êµ RAG ë°ì´í„° íŒŒì´í”„ë¼ì¸")
    print("="*70 + "\n")

    # í¬ë¡¤ë§í•  URL ì„¤ì •
    print("ğŸ“ í¬ë¡¤ë§í•  í˜ì´ì§€ ì„¤ì •\n")

    # ê¸°ë³¸ URL (ì˜ˆì‹œ)
    default_urls = [
        # í•™ì‚¬ê³µì§€
        'https://www.dyu.ac.kr/plaza/news/study-inform/',
        # í•™ì‚¬ì •ë³´
        'https://www.dyu.ac.kr/academic/',
        # ì¥í•™/í•™ìê¸ˆ
        'https://www.dyu.ac.kr/life/scholarship/',
        # ì·¨ì—…ì •ë³´
        'https://www.dyu.ac.kr/employ/',
    ]

    print("ê¸°ë³¸ í¬ë¡¤ë§ ëŒ€ìƒ:")
    for i, url in enumerate(default_urls, 1):
        print(f"  {i}. {url}")

    print("\nì˜µì…˜:")
    print("  1) ê¸°ë³¸ URL ì‚¬ìš©")
    print("  2) URL ì§ì ‘ ì…ë ¥")
    print("  3) íŒŒì¼ì—ì„œ URL ë¡œë“œ (urls.txt)")

    choice = input("\nì„ íƒ (1-3): ").strip()

    if choice == '2':
        urls = []
        print("\nURLì„ ì…ë ¥í•˜ì„¸ìš” (ë¹ˆ ì¤„ ì…ë ¥ ì‹œ ì¢…ë£Œ):")
        while True:
            url = input("URL: ").strip()
            if not url:
                break
            urls.append(url)
    elif choice == '3':
        try:
            with open('urls.txt', 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
            print(f"\nâœ… {len(urls)}ê°œ URL ë¡œë“œë¨")
        except FileNotFoundError:
            print("\nâŒ urls.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ URL ì‚¬ìš©")
            urls = default_urls
    else:
        urls = default_urls

    if not urls:
        print("\nâŒ URLì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # OCR ì„¤ì •
    print("\n" + "="*70)
    print("ğŸ” OCR ì„¤ì •")
    print("="*70 + "\n")

    print("ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    print("  y) ì˜ˆ (OpenAI Vision ì‚¬ìš© - ìœ ë£Œ)")
    print("  n) ì•„ë‹ˆì˜¤ (ì´ë¯¸ì§€ ê±´ë„ˆë›°ê¸°)")

    ocr_choice = input("\nì„ íƒ (y/n): ").strip().lower()
    ocr_enabled = (ocr_choice == 'y')

    ocr_model = "gpt-4o-mini"
    if ocr_enabled:
        print("\nì‚¬ìš©í•  ëª¨ë¸:")
        print("  1) gpt-4o-mini (ì¶”ì²œ - ì €ë ´)")
        print("  2) gpt-4o (ê³ ì„±ëŠ¥ - ë¹„ìŒˆ)")

        model_choice = input("\nì„ íƒ (1-2): ").strip()
        if model_choice == '2':
            ocr_model = "gpt-4o"

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    pipeline = Pipeline(output_dir="output")
    pipeline.run(urls, ocr_enabled=ocr_enabled, ocr_model=ocr_model)


if __name__ == "__main__":
    main()

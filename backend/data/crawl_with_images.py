"""
ì´ë¯¸ì§€ í¬í•¨ ì›¹ í¬ë¡¤ëŸ¬

ê¸°ëŠ¥:
1. ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
2. ì´ë¯¸ì§€ URL ìˆ˜ì§‘ ë° ë‹¤ìš´ë¡œë“œ
3. Base64 ì¸ì½”ë”© (OpenAI API ì „ì†¡ìš©)
"""

import requests
from bs4 import BeautifulSoup
import base64
from typing import Dict, List
from urllib.parse import urljoin, urlparse
import time


def is_valid_image(img_url: str) -> bool:
    """
    ìœ íš¨í•œ ì´ë¯¸ì§€ URLì¸ì§€ í™•ì¸

    Args:
        img_url: ì´ë¯¸ì§€ URL

    Returns:
        ìœ íš¨ ì—¬ë¶€
    """
    # ì´ë¯¸ì§€ í™•ì¥ì ì²´í¬
    valid_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp']

    # URL íŒŒì‹±
    parsed = urlparse(img_url.lower())
    path = parsed.path

    # í™•ì¥ì ì²´í¬
    if any(path.endswith(ext) for ext in valid_extensions):
        return True

    # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„°ì— í™•ì¥ìê°€ ìˆì„ ìˆ˜ë„ ìˆìŒ
    if any(ext in img_url.lower() for ext in valid_extensions):
        return True

    return False


def download_image(img_url: str, timeout: int = 10) -> Dict:
    """
    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° Base64 ì¸ì½”ë”©

    Args:
        img_url: ì´ë¯¸ì§€ URL
        timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    Returns:
        ì´ë¯¸ì§€ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (url, data, size)
    """
    try:
        response = requests.get(img_url, timeout=timeout, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

        if response.status_code != 200:
            return None

        # ì´ë¯¸ì§€ í¬ê¸° ì²´í¬ (5MB ì´í•˜ë§Œ)
        content_length = len(response.content)
        if content_length > 5 * 1024 * 1024:  # 5MB
            print(f"    âš ï¸ ì´ë¯¸ì§€ ë„ˆë¬´ í¼ (ê±´ë„ˆëœ€): {content_length/1024/1024:.1f}MB")
            return None

        # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ì•„ì´ì½˜ì¼ ê°€ëŠ¥ì„± (10KB ì´í•˜ ì œì™¸)
        if content_length < 10 * 1024:  # 10KB
            return None

        # Base64 ì¸ì½”ë”©
        img_b64 = base64.b64encode(response.content).decode('utf-8')

        return {
            'url': img_url,
            'data': img_b64,
            'size': content_length
        }

    except Exception as e:
        print(f"    âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {img_url[:50]}... - {e}")
        return None


def extract_images(soup: BeautifulSoup, base_url: str) -> List[Dict]:
    """
    í˜ì´ì§€ì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ ì¶”ì¶œ

    Args:
        soup: BeautifulSoup ê°ì²´
        base_url: ê¸°ë³¸ URL (ìƒëŒ€ê²½ë¡œ ë³€í™˜ìš©)

    Returns:
        ì´ë¯¸ì§€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    images = []
    img_tags = soup.find_all('img')

    print(f"  ğŸ“¸ {len(img_tags)}ê°œ ì´ë¯¸ì§€ íƒœê·¸ ë°œê²¬")

    for i, img in enumerate(img_tags, 1):
        # src ë˜ëŠ” data-src ì†ì„±ì—ì„œ URL ì¶”ì¶œ
        img_url = img.get('src') or img.get('data-src')

        if not img_url:
            continue

        # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
        img_url = urljoin(base_url, img_url)

        # ìœ íš¨ì„± ì²´í¬
        if not is_valid_image(img_url):
            continue

        # alt í…ìŠ¤íŠ¸ ì¶”ì¶œ
        alt_text = img.get('alt', '')

        print(f"    ë‹¤ìš´ë¡œë“œ ì¤‘ ({i}/{len(img_tags)}): {img_url[:50]}...")

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        img_data = download_image(img_url)

        if img_data:
            img_data['alt'] = alt_text
            images.append(img_data)
            print(f"    âœ… ì„±ê³µ ({img_data['size']/1024:.1f}KB)")

        # ì„œë²„ ë¶€í•˜ ë°©ì§€
        time.sleep(0.5)

    print(f"  âœ… ì´ {len(images)}ê°œ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")

    return images


def crawl_page_with_images(url: str) -> Dict:
    """
    ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)

    Args:
        url: í¬ë¡¤ë§í•  URL

    Returns:
        í˜ì´ì§€ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    print(f"\nğŸ” í¬ë¡¤ë§: {url}")

    try:
        # í˜ì´ì§€ ìš”ì²­
        response = requests.get(url, timeout=30, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        response.raise_for_status()

        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.content, 'lxml')

        # ì œëª© ì¶”ì¶œ
        title = soup.find('title')
        title = title.get_text().strip() if title else url

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ (bodyë§Œ)
        body = soup.find('body')
        if body:
            text = body.get_text()
        else:
            text = soup.get_text()

        # ì´ë¯¸ì§€ ì¶”ì¶œ
        images = extract_images(soup, url)

        return {
            'url': url,
            'title': title,
            'text': text,
            'content': text,  # clean_data.pyì™€ í˜¸í™˜ì„±
            'images': images,
            'image_count': len(images)
        }

    except Exception as e:
        print(f"  âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {
            'url': url,
            'title': url,
            'text': '',
            'content': '',
            'images': [],
            'image_count': 0,
            'error': str(e)
        }


def crawl_multiple_pages(urls: List[str]) -> List[Dict]:
    """
    ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§

    Args:
        urls: URL ë¦¬ìŠ¤íŠ¸

    Returns:
        í˜ì´ì§€ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    results = []
    total = len(urls)

    print(f"ğŸš€ ì´ {total}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘...\n")

    for i, url in enumerate(urls, 1):
        print(f"\n{'='*60}")
        print(f"ì§„í–‰: {i}/{total} ({i/total*100:.1f}%)")
        print(f"{'='*60}")

        data = crawl_page_with_images(url)
        results.append(data)

        # ì„œë²„ ë¶€í•˜ ë°©ì§€
        if i < total:
            time.sleep(2)

    # í†µê³„
    total_images = sum(d['image_count'] for d in results)
    successful = sum(1 for d in results if 'error' not in d)

    print(f"\n{'='*60}")
    print(f"ğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ì„±ê³µ: {successful}/{total}ê°œ í˜ì´ì§€")
    print(f"ì´ ì´ë¯¸ì§€: {total_images}ê°œ")
    print(f"{'='*60}\n")

    return results


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ URL (í•™ì‚¬ê³µì§€ í˜ì´ì§€)
    test_urls = [
        'https://www.dyu.ac.kr/plaza/news/study-inform/',
    ]

    results = crawl_multiple_pages(test_urls)

    # ê²°ê³¼ ì¶œë ¥
    for result in results:
        print(f"\nURL: {result['url']}")
        print(f"ì œëª©: {result['title'][:50]}...")
        print(f"í…ìŠ¤íŠ¸ ê¸¸ì´: {len(result['text'])}ì")
        print(f"ì´ë¯¸ì§€ ê°œìˆ˜: {result['image_count']}ê°œ")

        if result['images']:
            print("\nì´ë¯¸ì§€:")
            for img in result['images'][:3]:  # ì²˜ìŒ 3ê°œë§Œ
                print(f"  - {img['url'][:70]}...")
                print(f"    Alt: {img['alt'][:50]}..." if img['alt'] else "    Alt: (ì—†ìŒ)")
                print(f"    í¬ê¸°: {img['size']/1024:.1f}KB")

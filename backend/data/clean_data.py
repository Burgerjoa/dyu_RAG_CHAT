"""
ë°ì´í„° ì •ì œ ìŠ¤í¬ë¦½íŠ¸

ê¸°ëŠ¥:
1. HTML íƒœê·¸ ì œê±°
2. ì¤‘ë³µ URL ì œê±°
3. ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì •ë¦¬
4. ë„ˆë¬´ ì§§ì€ ë¬¸ì„œ í•„í„°ë§
"""

import json
import re
from typing import List, Dict
from collections import Counter
from bs4 import BeautifulSoup


def clean_html(content: str) -> str:
    """
    HTML íƒœê·¸ë¥¼ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì •ì œ

    Args:
        content: ì›ë³¸ HTML ë˜ëŠ” í…ìŠ¤íŠ¸

    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
    soup = BeautifulSoup(content, 'lxml')

    # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì™„ì „ ì œê±°
    for tag in soup(['script', 'style', 'nav', 'footer', 'header',
                     'iframe', 'noscript', 'meta', 'link']):
        tag.decompose()

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
    text = soup.get_text()

    # ì¤‘ë³µ ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±°
    lines = [line.strip() for line in text.splitlines()]
    lines = [line for line in lines if line]  # ë¹ˆ ì¤„ ì œê±°

    # ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì œê±° (ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´ ë“±)
    # ì˜ˆ: "Previous tab Next tab" ê°™ì€ íŒ¨í„´
    lines = [line for line in lines if not re.match(r'^(Previous|Next|tab|\||Â»|Â«)+\s*(Previous|Next|tab|\||Â»|Â«)*$', line)]

    # íŠ¹ìˆ˜ íŒ¨í„´ ì œê±°
    common_patterns = [
        r'^ëª©ë¡ë³´ê¸°$',
        r'^ì¸ì‡„$',
        r'^ì¢‹ì•„ìš” \d+ ì‹«ì–´ìš” \d+$',
        r'^Read more$',
        r'^ì¡°íšŒ \d+$',
    ]

    filtered_lines = []
    for line in lines:
        skip = False
        for pattern in common_patterns:
            if re.match(pattern, line.strip()):
                skip = True
                break
        if not skip:
            filtered_lines.append(line)

    # ìµœì¢… í…ìŠ¤íŠ¸ ì¡°í•©
    text = '\n'.join(filtered_lines)

    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)

    # 3ê°œ ì´ìƒ ì—°ì†ëœ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ
    text = re.sub(r'\n{3,}', '\n\n', text)

    return text.strip()


def remove_duplicates(data: List[Dict]) -> List[Dict]:
    """
    URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ìµœì‹  ê²ƒ ìœ ì§€)

    Args:
        data: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Returns:
        ì¤‘ë³µ ì œê±°ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    seen_urls = {}

    for item in data:
        url = item['url']
        # URLì—ì„œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°í•˜ì—¬ ê¸°ë³¸ URLë§Œ ë¹„êµ
        base_url = url.split('?')[0]

        if base_url not in seen_urls:
            seen_urls[base_url] = item
        else:
            # ë” ê¸´ ì»¨í…ì¸ ë¥¼ ê°€ì§„ ê²ƒì„ ì„ íƒ
            if len(item['content']) > len(seen_urls[base_url]['content']):
                seen_urls[base_url] = item

    return list(seen_urls.values())


def filter_low_quality(data: List[Dict], min_length: int = 50) -> List[Dict]:
    """
    í’ˆì§ˆì´ ë‚®ì€ ë¬¸ì„œ í•„í„°ë§

    Args:
        data: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        min_length: ìµœì†Œ ì½˜í…ì¸  ê¸¸ì´

    Returns:
        í•„í„°ë§ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    filtered = []

    for item in data:
        content = item['content']

        # ë„ˆë¬´ ì§§ì€ ë¬¸ì„œ ì œì™¸
        if len(content) < min_length:
            continue

        # ì˜ë¯¸ ì—†ëŠ” ë¬¸ì„œ ì œì™¸ (ê±°ì˜ ìˆ«ìë§Œ ìˆê±°ë‚˜)
        if len(re.findall(r'\d', content)) / len(content) > 0.5:
            continue

        # ë°˜ë³µë˜ëŠ” ë¬¸ìê°€ ë„ˆë¬´ ë§ì€ ê²½ìš° ì œì™¸
        if len(set(content)) / len(content) < 0.1:
            continue

        filtered.append(item)

    return filtered


def print_statistics(original_data: List[Dict], cleaned_data: List[Dict]):
    """
    ë°ì´í„° ì •ì œ ì „í›„ í†µê³„ ì¶œë ¥

    Args:
        original_data: ì›ë³¸ ë°ì´í„°
        cleaned_data: ì •ì œëœ ë°ì´í„°
    """
    print("\n" + "="*60)
    print("ğŸ“Š ë°ì´í„° ì •ì œ í†µê³„")
    print("="*60)

    # ê¸°ë³¸ í†µê³„
    print(f"\nì›ë³¸ ë¬¸ì„œ ìˆ˜: {len(original_data):,}ê°œ")
    print(f"ì •ì œ í›„ ë¬¸ì„œ ìˆ˜: {len(cleaned_data):,}ê°œ")
    print(f"ì œê±°ëœ ë¬¸ì„œ: {len(original_data) - len(cleaned_data):,}ê°œ "
          f"({(1 - len(cleaned_data)/len(original_data))*100:.1f}% ê°ì†Œ)")

    # ì»¨í…ì¸  ê¸¸ì´ í†µê³„
    original_lengths = [len(item['content']) for item in original_data]
    cleaned_lengths = [len(item['content']) for item in cleaned_data]

    print(f"\ní‰ê·  ì½˜í…ì¸  ê¸¸ì´:")
    print(f"  ì›ë³¸: {sum(original_lengths)/len(original_lengths):.0f}ì")

    if cleaned_lengths:
        print(f"  ì •ì œ í›„: {sum(cleaned_lengths)/len(cleaned_lengths):.0f}ì")
    else:
        print(f"  ì •ì œ í›„: 0ì (ëª¨ë“  ë°ì´í„°ê°€ í•„í„°ë§ë¨)")

    total_original = sum(original_lengths)
    total_cleaned = sum(cleaned_lengths) if cleaned_lengths else 0

    print(f"\nì´ í…ìŠ¤íŠ¸ í¬ê¸°:")
    print(f"  ì›ë³¸: {total_original/1024/1024:.2f} MB")
    print(f"  ì •ì œ í›„: {total_cleaned/1024/1024:.2f} MB")

    if total_original > 0:
        print(f"  ê°ì†Œ: {(1 - total_cleaned/total_original)*100:.1f}%")

    # URL ë„ë©”ì¸ ë¶„í¬
    if cleaned_data:
        print(f"\nìƒìœ„ URL ë„ë©”ì¸:")
        domains = [item['url'].split('/')[2] if len(item['url'].split('/')) > 2 else item['url']
                   for item in cleaned_data]
        domain_counts = Counter(domains).most_common(5)
        for domain, count in domain_counts:
            print(f"  {domain}: {count}ê°œ")
    else:
        print(f"\nâš ï¸ ê²½ê³ : ì •ì œ í›„ ë‚¨ì€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!")

    print("="*60 + "\n")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    input_file = '111.json'
    output_file = '111_cleaned.json'

    print(f"ğŸš€ ë°ì´í„° ì •ì œ ì‹œì‘: {input_file}")

    # 1. ë°ì´í„° ë¡œë“œ
    print(f"\n1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ì¤‘...")
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"   âœ… {len(data):,}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    except FileNotFoundError:
        print(f"   âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return
    except json.JSONDecodeError as e:
        print(f"   âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return

    original_data = data.copy()

    # 2. ì¤‘ë³µ ì œê±°
    print(f"\n2ï¸âƒ£ ì¤‘ë³µ URL ì œê±° ì¤‘...")
    before_count = len(data)
    data = remove_duplicates(data)
    after_count = len(data)
    print(f"   âœ… {before_count - after_count:,}ê°œ ì¤‘ë³µ ì œê±° ({after_count:,}ê°œ ë‚¨ìŒ)")

    # 3. HTML ì •ì œ
    print(f"\n3ï¸âƒ£ HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ ì¤‘...")
    for i, item in enumerate(data):
        if i % 500 == 0:
            print(f"   ì§„í–‰: {i}/{len(data)} ({i/len(data)*100:.1f}%)")
        item['content'] = clean_html(item['content'])
    print(f"   âœ… ëª¨ë“  ë¬¸ì„œ ì •ì œ ì™„ë£Œ")

    # 4. ì €í’ˆì§ˆ ë¬¸ì„œ í•„í„°ë§
    print(f"\n4ï¸âƒ£ ì €í’ˆì§ˆ ë¬¸ì„œ í•„í„°ë§ ì¤‘...")
    before_count = len(data)
    data = filter_low_quality(data, min_length=50)
    after_count = len(data)
    print(f"   âœ… {before_count - after_count:,}ê°œ ë¬¸ì„œ ì œê±° ({after_count:,}ê°œ ë‚¨ìŒ)")

    # 5. í†µê³„ ì¶œë ¥
    print_statistics(original_data, data)

    # 6. ì €ì¥
    print(f"5ï¸âƒ£ ì •ì œëœ ë°ì´í„° ì €ì¥ ì¤‘: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"   âœ… ì €ì¥ ì™„ë£Œ!")

    # 7. ìƒ˜í”Œ ì¶œë ¥
    print(f"\n6ï¸âƒ£ ì •ì œ ì „í›„ ë¹„êµ (ì²« ë²ˆì§¸ ë¬¸ì„œ):")
    print("\n" + "-"*60)
    print("ğŸ“„ ì›ë³¸:")
    print("-"*60)
    print(original_data[0]['content'][:300] + "...")

    print("\n" + "-"*60)
    print("âœ¨ ì •ì œ í›„:")
    print("-"*60)
    # ì •ì œëœ ë°ì´í„°ì—ì„œ ê°™ì€ URL ì°¾ê¸°
    cleaned_sample = next((d for d in data if d['url'] == original_data[0]['url']), data[0])
    print(cleaned_sample['content'][:300] + "...")
    print("-"*60)

    print(f"\nğŸ‰ ë°ì´í„° ì •ì œ ì™„ë£Œ! {output_file} íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    print(f"   ë‹¤ìŒ ë‹¨ê³„: backend/rag_system.pyì—ì„œ data_path='data/111_cleaned.json'ìœ¼ë¡œ ë³€ê²½")


if __name__ == "__main__":
    main()
